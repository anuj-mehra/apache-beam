
from airflow import DAG
from airflow.sensors.filesystem import FileSensor
from airflow.operators.python import PythonOperator
from datetime import datetime, timedelta
import os

# Define default arguments for the DAG
default_args = {
    'start_date': datetime(2024, 1, 1),
    'retries': 1,
    'retry_delay': timedelta(minutes=5),
}

# File location to monitor (e.g., a Cloud Storage bucket mounted in Composer environment)
FILE_PATH = '/path/to/your/file.txt'

def read_and_process_file(file_path):
    # Check if file exists, read content, and process the file
    if os.path.exists(file_path):
        with open(file_path, 'r') as file:
            content = file.read()
            # Add your processing logic here
            print(f"File content: {content}")
    else:
        raise FileNotFoundError(f"{file_path} not found.")

with DAG('file_polling_workflow', default_args=default_args, schedule_interval='@once') as dag:

    # Task 1: Poll the location to check if the file exists
    file_sensor_task = FileSensor(
        task_id='wait_for_file',
        filepath=FILE_PATH,
        poke_interval=30,  # Check every 30 seconds
        timeout=600,       # Timeout after 10 minutes
        mode='poke'
    )

    # Task 2: Process the file content once it's available
    process_file_task = PythonOperator(
        task_id='process_file',
        python_callable=read_and_process_file,
        op_kwargs={'file_path': FILE_PATH}
    )

    # Task 3: Further tasks can be defined here
    def task3_processing():
        print("Task 3 is running after file processing.")

    task3 = PythonOperator(
        task_id='task3',
        python_callable=task3_processing
    )

    # Set up task dependencies
    file_sensor_task >> process_file_task >> task3


